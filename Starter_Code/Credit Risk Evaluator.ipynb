{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Risk Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve the Data\n",
    "\n",
    "The data is located in the Challenge Files Folder:\n",
    "\n",
    "* `lending_data.csv`\n",
    "\n",
    "Import the data using Pandas. Display the resulting dataframe to confirm the import was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_size</th>\n",
       "      <th>interest_rate</th>\n",
       "      <th>borrower_income</th>\n",
       "      <th>debt_to_income</th>\n",
       "      <th>num_of_accounts</th>\n",
       "      <th>derogatory_marks</th>\n",
       "      <th>total_debt</th>\n",
       "      <th>loan_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10700.0</td>\n",
       "      <td>7.672</td>\n",
       "      <td>52800</td>\n",
       "      <td>0.431818</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>22800</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8400.0</td>\n",
       "      <td>6.692</td>\n",
       "      <td>43600</td>\n",
       "      <td>0.311927</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>13600</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9000.0</td>\n",
       "      <td>6.963</td>\n",
       "      <td>46100</td>\n",
       "      <td>0.349241</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>16100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10700.0</td>\n",
       "      <td>7.664</td>\n",
       "      <td>52700</td>\n",
       "      <td>0.430740</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>22700</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10800.0</td>\n",
       "      <td>7.698</td>\n",
       "      <td>53000</td>\n",
       "      <td>0.433962</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>23000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   loan_size  interest_rate  borrower_income  debt_to_income  num_of_accounts  \\\n",
       "0    10700.0          7.672            52800        0.431818                5   \n",
       "1     8400.0          6.692            43600        0.311927                3   \n",
       "2     9000.0          6.963            46100        0.349241                3   \n",
       "3    10700.0          7.664            52700        0.430740                5   \n",
       "4    10800.0          7.698            53000        0.433962                5   \n",
       "\n",
       "   derogatory_marks  total_debt  loan_status  \n",
       "0                 1       22800            0  \n",
       "1                 0       13600            0  \n",
       "2                 0       16100            0  \n",
       "3                 1       22700            0  \n",
       "4                 1       23000            0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the data\n",
    "df = pd.read_csv(\"Resources//lending_data.csv\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Model Performance\n",
    "\n",
    "You will be creating and comparing two models on this data: a Logistic Regression, and a Random Forests Classifier. Before you create, fit, and score the models, make a prediction as to which model you think will perform better. You do not need to be correct! \n",
    "\n",
    "Write down your prediction in the designated cells in your Jupyter Notebook, and provide justification for your educated guess."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I believe the Logistic Regression will perform better since most/all of the data is numeric and not categorical/classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the Data into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create X and Y\n",
    "X= df.drop(columns=['loan_status'])\n",
    "Y= df['loan_status']\n",
    "\n",
    "# Split the data into X_train, X_test, y_train, y_test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create, Fit and Compare Models\n",
    "\n",
    "Create a Logistic Regression model, fit it to the data, and print the model's score. Do the same for a Random Forest Classifier. You may choose any starting hyperparameters you like. \n",
    "\n",
    "Which model performed better? How does that compare to your prediction? Write down your results and thoughts in the designated markdown cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression Training Score: 0.9920553033429633\n",
      "Regression Testing Score: 0.9917457697069748\n"
     ]
    }
   ],
   "source": [
    "# Train a Logistic Regression model and print the model score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "reg = LogisticRegression(random_state=0).fit(X_train, Y_train)\n",
    "print(f\"Regression Training Score: {reg.score(X_train, Y_train)}\")\n",
    "print(f\"Regression Testing Score: {reg.score(X_test, Y_test)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy itself doesn't tell us a lot about a model's performance so I'm also going to run a classification report to find precision and recall for both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[18666,    94],\n",
       "       [   66,   558]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "Y_true = Y_test\n",
    "Y_pred = reg.predict(X_test)\n",
    "confusion_matrix(Y_true, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positives (TP): 558\n",
      "True negatives (TN): 18666\n",
      "False positives (FP): 94\n",
      "False negatives (FN): 66\n",
      "Accuracy: 0.9917457697069748\n",
      "Precision: 0.8558282208588958\n",
      "Recall/Sensitivity: 0.8942307692307693\n",
      "f1 Score: 0.8746081504702194\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(Y_true, Y_pred).ravel()\n",
    "print(f\"True positives (TP): {tp}\")\n",
    "print(f\"True negatives (TN): {tn}\")\n",
    "print(f\"False positives (FP): {fp}\")\n",
    "print(f\"False negatives (FN): {fn}\")\n",
    "\n",
    "accuracy = (tp + tn) / (tp + fp + tn + fn)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "precision = tp / (tp + fp)\n",
    "print(f\"Precision: {precision}\")\n",
    "\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(f\"Recall/Sensitivity: {sensitivity}\")\n",
    "\n",
    "f1 = 2*precision*sensitivity / (precision + sensitivity)\n",
    "print(f\"f1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     18760\n",
      "           1       0.86      0.89      0.87       624\n",
      "\n",
      "    accuracy                           0.99     19384\n",
      "   macro avg       0.93      0.94      0.94     19384\n",
      "weighted avg       0.99      0.99      0.99     19384\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(Y_true, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forest Training Score: 0.9973173751547668\n",
      "Forest Testing Score: 0.9921068922822947\n"
     ]
    }
   ],
   "source": [
    "# Train a Random Forest Classifier model and print the model score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forst = RandomForestClassifier(random_state=0).fit(X_train, Y_train)\n",
    "print(f\"Forest Training Score: {forst.score(X_train, Y_train)}\")\n",
    "print(f\"Forest Testing Score: {forst.score(X_test, Y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[18667,    93],\n",
       "       [   60,   564]], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "Y_true = Y_test\n",
    "Y_pred = forst.predict(X_test)\n",
    "confusion_matrix(Y_true, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positives (TP): 564\n",
      "True negatives (TN): 18667\n",
      "False positives (FP): 93\n",
      "False negatives (FN): 60\n",
      "Accuracy: 0.9921068922822947\n",
      "Precision: 0.8584474885844748\n",
      "Sensitivity: 0.9038461538461539\n",
      "f1 Score: 0.8805620608899296\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(Y_true, Y_pred).ravel()\n",
    "print(f\"True positives (TP): {tp}\")\n",
    "print(f\"True negatives (TN): {tn}\")\n",
    "print(f\"False positives (FP): {fp}\")\n",
    "print(f\"False negatives (FN): {fn}\")\n",
    "\n",
    "accuracy = (tp + tn) / (tp + fp + tn + fn) \n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "precision = tp / (tp + fp)\n",
    "print(f\"Precision: {precision}\")\n",
    "\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(f\"Sensitivity: {sensitivity}\")\n",
    "\n",
    "f1 = 2*precision*sensitivity / (precision + sensitivity)\n",
    "print(f\"f1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     18760\n",
      "           1       0.86      0.90      0.88       624\n",
      "\n",
      "    accuracy                           0.99     19384\n",
      "   macro avg       0.93      0.95      0.94     19384\n",
      "weighted avg       0.99      0.99      0.99     19384\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(Y_true, Y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Which model performed better? How does that compare to your prediction? Replace the text in this markdown cell with your answers to these questions.*\n",
    "It seems that the Random Forest Classifier performed better than the Logistic Regression model, but only by a hair. In precision, recall, and f1-score, RFC is just slightly closer to the ideal score of 1.0 than those of LR. RFC also had slightly more accurate \"results\" where there were fewer false positives and false negatives than LR's results.\n",
    "\n",
    "My prediction did not turn out to be correct, I would be interested in how the two compare with different data sets and types of data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "c0482787fa3bd72135f2c71a85a143c792c73cbdeb9cafecb3b1b910d629ab41"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
